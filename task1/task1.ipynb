{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import cmath\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 宇宙的答案\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data and EDA\n",
    "\n",
    "train中有4列，phraseid和sentenceid无意义，可以舍弃\n",
    "\n",
    "test中仅3列，无sentiment，需要我们自己预测\n",
    "\n",
    "无需特殊处理数据清洗后产生的的None或者空列表，他们原本都是无意义的停止词，一律按照neutral(中性)处理\n",
    "\n",
    "标签有如下几种\n",
    "  + 0 - negative\n",
    "  + 1 - somewhat negative\n",
    "  + 2 - neutral\n",
    "  + 3 - somewhat positive\n",
    "  + 4 - positive\n",
    "\n",
    "要求中提到要使用BOW(short of bag of word)和N-gram进行文本表示，但是N-gram在N=1时本身就是BOW，所以只要写一个可以调节N的N-gram tokenization即可。\n",
    "\n",
    "N-gram文本表示：思路与词袋模型相同，不考虑词的顺序，将N个词作为一组放进词袋里，当N=1时实际上就是BOW。这种方法形成的词向量的长度为字典的长度，每个值都是当前位置代表的词组出现的次数。使用可以过滤一部分低频率的词组来降低字典维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 156060 entries, 0 to 156059\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count   Dtype \n",
      "---  ------     --------------   ----- \n",
      " 0   Phrase     156060 non-null  object\n",
      " 1   Sentiment  156060 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.4+ MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train=pd.read_csv(\"train.tsv\", delimiter=\"\\t\").drop(columns=[\"PhraseId\",\"SentenceId\"])\n",
    "test=pd.read_csv(\"test.tsv\", delimiter=\"\\t\").drop(columns=[\"PhraseId\",\"SentenceId\"])\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>An intermittently pleasing but mostly routine ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>An intermittently pleasing but mostly routine ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>An</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>intermittently pleasing but mostly routine effort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>intermittently pleasing but mostly routine</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Phrase\n",
       "0  An intermittently pleasing but mostly routine ...\n",
       "1  An intermittently pleasing but mostly routine ...\n",
       "2                                                 An\n",
       "3  intermittently pleasing but mostly routine effort\n",
       "4         intermittently pleasing but mostly routine"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Phrase  Sentiment\n",
       "0  A series of escapades demonstrating the adage ...          1\n",
       "1  A series of escapades demonstrating the adage ...          2\n",
       "2                                           A series          2\n",
       "3                                                  A          2\n",
       "4                                             series          2"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         A series of escapades demonstrating the adage ...\n",
       "1         A series of escapades demonstrating the adage ...\n",
       "2                                                  A series\n",
       "3                                                         A\n",
       "4                                                    series\n",
       "                                ...                        \n",
       "156055                                            Hearst 's\n",
       "156056                            forced avuncular chortles\n",
       "156057                                   avuncular chortles\n",
       "156058                                            avuncular\n",
       "156059                                             chortles\n",
       "Name: Phrase, Length: 156060, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"Phrase\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\\Dictionary\\Vectorization\\Feature selection\n",
    "\n",
    "根据zipf定律，大部分的词汇出现的频率很少，所以将这些词汇根据出现频率删除，由此来进行特征选择。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参考代码，refer to https://thinkingneuron.com/how-to-generate-n-grams-in-python/\n",
    "# # Creating a function to generate N-Grams\n",
    "# def generate_ngrams(text, WordsToCombine):\n",
    "#      words = text.split()\n",
    "#      output = []  \n",
    "#      for i in range(len(words)- WordsToCombine+1):\n",
    "#          output.append(words[i:i+WordsToCombine])\n",
    "#      return output\n",
    " \n",
    "# # Calling the function\n",
    "# generate_ngrams(text='this is a very good book to study', WordsToCombine=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['escapadesdemonstrating', 'demonstratingadage', 'adagegood', 'goodgoose']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def tokenization_to_ngram(sentence,n=1):\n",
    "    '''\n",
    "    将句子转化为token,去除停止词,并返回用于n-gram语言建模的特征\n",
    "    '''\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # print(type(sentence))\n",
    "    words = re.sub(\"[^\\w]\", \" \",  sentence).split() \n",
    "    filtered_sentence=[w.lower() for w in words if not w in stop_words]\n",
    "    output=[]\n",
    "    if(n!=1):\n",
    "        if(len(filtered_sentence)<n):\n",
    "            # 对于n>len(filtered_sentence)的情况，直接将句子中所有的词拼接\n",
    "            tmp=''\n",
    "            for i in range(len(filtered_sentence)):\n",
    "                tmp=tmp+filtered_sentence[i]\n",
    "            output.append(tmp)\n",
    "            return output\n",
    "        else:\n",
    "            # 对于其他情况，将句子分解为n个词一份\n",
    "            for i in range(len(filtered_sentence)-n+1):\n",
    "                # 这一步是将n个单词拼在一起作为一个单词，这样的话可以视作一个单词，方便一会儿进行哈希\n",
    "                tmp=filtered_sentence[i]\n",
    "                for t in range(1,n):\n",
    "                    tmp+=filtered_sentence[i+t]\n",
    "                output.append(tmp)\n",
    "            return output\n",
    "    else:\n",
    "        return filtered_sentence\n",
    "\n",
    "tokenization_to_ngram(train[\"Phrase\"][7],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictionarymake(df,colums,lim=10):\n",
    "    '''制作字典'''\n",
    "    corpus=[]\n",
    "    for ls in df[colums]:\n",
    "        for w in ls:\n",
    "            if( w not in corpus):\n",
    "                corpus.append(w)\n",
    "    # 特征选择，根据出现频率决定是否加入字典\n",
    "    length=len(corpus)\n",
    "    tmp=np.zeros(length)\n",
    "    for ls in df[colums]:\n",
    "        for w in ls:\n",
    "            tmp[corpus.index(w)]+=1\n",
    "    dictionary={}\n",
    "    t=0\n",
    "    for i,word in enumerate(corpus):\n",
    "        if(tmp[i]>lim):\n",
    "            dictionary[word]=t\n",
    "            t+=1\n",
    "            \n",
    "    return dictionary,t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(ls,dict,vector_len):\n",
    "    '''将句子根据字典转化为定长向量'''\n",
    "    # 使用16位浮点数，防止太长爆掉\n",
    "    vec=np.zeros(vector_len)\n",
    "    key=list(dict.keys())\n",
    "    for w in ls:\n",
    "        if(w in key):\n",
    "            vec[dict[w]]+=1\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### softmax\n",
    "\n",
    "Logstic回归是softmax在2分类下的特殊情况，所以我们仅实现softmxa。\n",
    "\n",
    "由于是分类任务，损失函数使用交叉熵。\n",
    "\n",
    "使用随机梯度下降进行优化，尽管softmax有闭式解，但是这里的矩阵太大，最好使用梯度下降的方法优化。\n",
    "\n",
    "对于softmax/logistics回归来说，假设$c_k$表示类别k，X表示输入，$W_i$表示每个类别对应的权重，则其公式如下所示：\n",
    "\n",
    "$P(C=c_k |x=X)=softmax(X)=\\frac{e^{W_{k}^TX}}{\\sum_{i=1}^{K}{e^{W_{i}^TX}}}$\n",
    "\n",
    "对于Logistics回归，另一种更严谨的公式是这样的（注意分母没有第K类,此时的公式由两两分类下的对数线性回归严格的推出）：\n",
    "\n",
    " + 对于第K类$P(C=c_K |x=X)=logistics(X)=\\frac{1}{1+\\sum_{i=1}^{K-1}{e^{W_{i}^TX}}}$\n",
    " + 对于其余的K-1类，$P(C=c_{k-1} |x=X)=logistics(X)=\\frac{e^{W_{k-1}^TX}}{1+\\sum_{i=1}^{K-1}{e^{W_{i}^TX}}}$\n",
    "\n",
    "这是由于下面的两个公式没有同时在分子和分母上乘以$e^{W_{k}^TX}$，由于分子分母同时变大，所以对最终的拟合结果并无影响。同时，上面的式子要对**K**个类别计算参数W，而下面的式子每次分类时需要对**K-1**个参数进行计算。\n",
    "\n",
    "虽然如此，这并不是说下面的这个式子计算的参数量要少，两者其实计算的参数量是一致的。问题就在于如果采用了下面的这个式子进行计算，我们需要对每个类别计算K-1个参数，每两个类之间的参数一定有K-2个重合，同时每个类都保留一个自己独特的参数。这样两两对比K次，我们就得到了我们所需要的K个独特的参数，这个参数量和上面的式子保持一致。\n",
    "\n",
    "可以看到上面的这个公式由于更加通用，更加的便于计算，所以更加常见，但是正式的logistics回归其实是下面的这种形式。\n",
    "\n",
    "**最后需要注意的一点是**，将K个类别完全分开，我们至少需要K个超平面，因此softmax回归等线性分类方法其本质是一种一对其余的分类思想。所以若进行K分类，其权重w最好为(m,k)，也就是对每个类都有一个单独的权重$w_i$将其与其他类分开，因此我们需要将(n,1)的标签y扩展到(n,k)的编码器labelencoder。\n",
    "\n",
    "参考链接：https://esl.hohoweiya.xyz/04-Linear-Methods-for-Classification/4.4-Logistic-Regression/index.html#_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, RepeatedStratifiedKFold\n",
    "\n",
    "from numpy.random import shuffle\n",
    "\n",
    "# 使用softmax进行分类\n",
    "class softmax():\n",
    "    def __init__(self,a=0.1,batchsize=64,iteration=1) -> None:\n",
    "        '''\n",
    "        X->(n,m):n个m维样例 \n",
    "        y->(n,k):n个样例,每个样例中有K个值,对应K个类别\n",
    "        w->(m,k):将x映射到k个类别中\n",
    "        a:学习率\n",
    "        iteration:迭代次数\n",
    "        batchsize:单个batch的大小\n",
    "        '''\n",
    "        self.w=None\n",
    "        self.a=a\n",
    "        self.batchsize=batchsize\n",
    "        self.iter=iteration\n",
    "        self.loss=[]\n",
    "        self.k=2\n",
    "\n",
    "    def softmax_fun(self,X):\n",
    "        tmp=X@self.w\n",
    "        # 防止指数计算得到的结果过大\n",
    "        tmp-=np.max(tmp,axis=1,keepdims=True)\n",
    "        k=np.exp(tmp)/np.sum(np.exp(tmp),axis=1,keepdims=True)\n",
    "        return k\n",
    "\n",
    "\n",
    "    def labelencoder(self,y):\n",
    "        '''\n",
    "        将输入的标签根据类别数编码为特定的形状\n",
    "        '''\n",
    "        # numpy整数索引的应用\n",
    "        return np.eye(self.k)[y.reshape(-1)]\n",
    "\n",
    "    def labeldecoder(self,y):\n",
    "        '''将输入的K维标签映射回1维'''\n",
    "        tmp=np.argmax(y,axis=1)\n",
    "        a,_=y.shape\n",
    "        return tmp.reshape(a,1)\n",
    "\n",
    "    def loss_fun(self,y,y_hat):\n",
    "        epsilon = 1e-10  # 添加一个小的值以避免除以零\n",
    "        loss = -(y * np.log(y_hat + epsilon) + (1 - y) * np.log(1 - y_hat + epsilon))\n",
    "        return np.mean(loss)\n",
    "\n",
    "\n",
    "    def fit(self,X,y):\n",
    "        '''根据要求使用mini-batch的梯度下降进行优化'''\n",
    "        n,m=X.shape\n",
    "        # 将标签进行编码\n",
    "        self.k=len(np.unique(y))\n",
    "        y=self.labelencoder(y)\n",
    "        # 将X增广一列，以训练bias\n",
    "        X_aug=np.concatenate((X,np.ones((n,1))),axis=1)\n",
    "        # 多次调试找到的较优初始化方法\n",
    "        self.w=np.random.normal(loc=0,scale=0.5,size=(m+1,self.k))\n",
    "        # mini-batch SGD\n",
    "        batch=len(X_aug)//self.batchsize\n",
    "        for it in range(self.iter):\n",
    "            # 每轮训练开始时，打乱数据集\n",
    "            shuffle_index=np.random.permutation(len(X_aug))\n",
    "            X_=X_aug[shuffle_index]\n",
    "            y_=y[shuffle_index]    \n",
    "            for i in range(batch):\n",
    "                # 注意越界\n",
    "                if(i!=batch-1):\n",
    "                    begin=i*self.batchsize\n",
    "                    end=(i+1)*self.batchsize\n",
    "                else:\n",
    "                    # 最后一个batch，单独处理防止越界\n",
    "                    begin=i*self.batchsize\n",
    "                    end=len(X_aug)\n",
    "                batch_x=X_[begin:end]\n",
    "                batch_y=y_[begin:end]\n",
    "                # SGD\n",
    "                y_hat=self.softmax_fun(batch_x)\n",
    "                # 记录loss\n",
    "                loss=self.loss_fun(batch_y,y_hat)\n",
    "                self.loss.append(loss)\n",
    "                self.w+=self.a*batch_x.T@(batch_y-y_hat)/self.batchsize\n",
    "\n",
    "    def predict(self,X):\n",
    "        # 增广一列，加入bias\n",
    "        a,_=X.shape\n",
    "        X_aug=np.concatenate((X,np.ones((a,1))),axis=1)\n",
    "        # 输出预测时将标签解码\n",
    "        return self.labeldecoder(self.softmax_fun(X_aug))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算准确度\n",
    "def acc(y_hat,y):\n",
    "    return np.mean(y==y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将所有短语提取出来\n",
    "all_phrase_1=pd.concat([train[\"Phrase\"],test[\"Phrase\"]],axis=0,ignore_index=True)\\\n",
    "            .apply(tokenization_to_ngram,n=1)\\\n",
    "            .to_frame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 156060 entries, 0 to 156059\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   Phrase  156060 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 1.2+ MB\n"
     ]
    }
   ],
   "source": [
    "# 将训练集上的短语提取出来\n",
    "X_train_1=train[\"Phrase\"].apply(tokenization_to_ngram,n=1).to_frame()\n",
    "X_train_1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5268"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 制作对应的字典\n",
    "dict_1,len_v1=dictionarymake(all_phrase_1,colums=\"Phrase\",lim=30)\n",
    "len_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将数据转化为numpy数组的格式\n",
    "X1=X_train_1[\"Phrase\"]\\\n",
    "    .apply(vectorize,dict=dict_1,vector_len=len_v1)\\\n",
    "    .to_numpy()\n",
    "y=train[\"Sentiment\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1=np.stack(X1,axis=0)\n",
    "y=y.reshape(y.shape[0],1)\n",
    "# 划分测试集和验证集,验证集不需要太大\n",
    "val_size=0.1\n",
    "train_size=int(len(X1)*(1-val_size))\n",
    "X_train_1,y_train_1=X1[:train_size],y[:train_size]\n",
    "X_val_1,y_val_1=X1[train_size:],y[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.43252595155709345\n"
     ]
    }
   ],
   "source": [
    "\n",
    "soft=softmax(a=1e-3,iteration=5)\n",
    "soft.fit(X_train_1,y_train_1)\n",
    "y_pred_1=soft.predict(X_val_1)\n",
    "print(acc(y_pred_1,y_val_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38319844451037005\n"
     ]
    }
   ],
   "source": [
    "# 与sklearn中同类方法对比，所以我们的实现方法更好一点基本是正确的\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "ls=LogisticRegression()\n",
    "ls.fit(X_train_1,y_train_1)\n",
    "n=ls.predict(X_val_1)\n",
    "print(acc(y=y_val_1,y_hat=n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X1\n",
    "del dict_1\n",
    "del X_train_1,y_train_1\n",
    "del X_val_1,y_val_1\n",
    "del all_phrase_1\n",
    "del y_pred_1\n",
    "del soft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 156060 entries, 0 to 156059\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   Phrase  156060 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 1.2+ MB\n"
     ]
    }
   ],
   "source": [
    "# 以n=2时为例\n",
    "# 将所有短语提取出来\n",
    "all_phrase_2=pd.concat([train[\"Phrase\"],test[\"Phrase\"]],axis=0,ignore_index=True)\\\n",
    "            .apply(tokenization_to_ngram,n=2)\\\n",
    "            .to_frame()\n",
    "X_train_2=train[\"Phrase\"].apply(tokenization_to_ngram,n=2).to_frame()\n",
    "X_train_2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1003"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_2,len_v2=dictionarymake(all_phrase_2,colums=\"Phrase\",lim=30)\n",
    "len_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2=X_train_2[\"Phrase\"]\\\n",
    "    .apply(vectorize,dict=dict_2,vector_len=len_v2)\\\n",
    "    .to_numpy()\n",
    "y=train[\"Sentiment\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2=np.stack(X2,axis=0)\n",
    "y=y.reshape(y.shape[0],1)\n",
    "# 划分测试集和验证集,验证集不需要太大\n",
    "val_size=0.1\n",
    "train_size=int(len(X2)*(1-val_size))\n",
    "X_train_2,y_train_2=X2[:train_size],y[:train_size]\n",
    "X_val_2,y_val_2=X2[train_size:],y[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4812892477252339\n"
     ]
    }
   ],
   "source": [
    "soft=softmax(a=1e-3,iteration=5)\n",
    "soft.fit(X_train_2,y_train_2)\n",
    "y_pred_2=soft.predict(X_val_2)\n",
    "print(acc(y_pred_2,y_val_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4628495147446469\n"
     ]
    }
   ],
   "source": [
    "ls=LogisticRegression()\n",
    "ls.fit(X_train_2,y_train_2)\n",
    "n=ls.predict(X_val_2)\n",
    "print(acc(y=y_val_2,y_hat=n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X2\n",
    "del dict_2\n",
    "del X_train_2,y_train_2\n",
    "del X_val_2,y_val_2\n",
    "del all_phrase_2\n",
    "del y_pred_2\n",
    "del soft"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
