{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import cmath\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import sklearn\n",
    "import gensim as gm\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 156060 entries, 0 to 156059\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count   Dtype \n",
      "---  ------     --------------   ----- \n",
      " 0   Phrase     156060 non-null  object\n",
      " 1   Sentiment  156060 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.4+ MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train=pd.read_csv(\"train.tsv\", delimiter=\"\\t\").drop(columns=[\"PhraseId\",\"SentenceId\"])\n",
    "test=pd.read_csv(\"test.tsv\", delimiter=\"\\t\").drop(columns=[\"PhraseId\",\"SentenceId\"])\n",
    "\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>An intermittently pleasing but mostly routine ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>An intermittently pleasing but mostly routine ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>An</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>intermittently pleasing but mostly routine effort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>intermittently pleasing but mostly routine</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Phrase\n",
       "0  An intermittently pleasing but mostly routine ...\n",
       "1  An intermittently pleasing but mostly routine ...\n",
       "2                                                 An\n",
       "3  intermittently pleasing but mostly routine effort\n",
       "4         intermittently pleasing but mostly routine"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Phrase  Sentiment\n",
       "0  A series of escapades demonstrating the adage ...          1\n",
       "1  A series of escapades demonstrating the adage ...          2\n",
       "2                                           A series          2\n",
       "3                                                  A          2\n",
       "4                                             series          2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         A series of escapades demonstrating the adage ...\n",
       "1         A series of escapades demonstrating the adage ...\n",
       "2                                                  A series\n",
       "3                                                         A\n",
       "4                                                    series\n",
       "                                ...                        \n",
       "156055                                            Hearst 's\n",
       "156056                            forced avuncular chortles\n",
       "156057                                   avuncular chortles\n",
       "156058                                            avuncular\n",
       "156059                                             chortles\n",
       "Name: Phrase, Length: 156060, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"Phrase\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA\n",
    "\n",
    "train中有4列，phraseid和sentenceid无意义，可以舍弃\n",
    "\n",
    "test中仅3列，无sentiment，需要我们自己预测\n",
    "\n",
    "无需特殊处理数据清洗后产生的的None或者空列表，他们原本都是无意义的停止词，一律按照neutral(中性)处理\n",
    "\n",
    "标签有如下几种\n",
    "  + 0 - negative\n",
    "  + 1 - somewhat negative\n",
    "  + 2 - neutral\n",
    "  + 3 - somewhat positive\n",
    "  + 4 - positive\n",
    "\n",
    "要求中提到要使用BOW(short of bag of word)和N-gram进行文本表示，但是N-gram在N=1时本身就是BOW，所以只要写一个可以调节N的N-gram tokenization即可。\n",
    "\n",
    "N-gram文本表示：思路与词袋模型相同，不考虑词的顺序，将N个词作为一组放进词袋里，当N=1时实际上就是BOW。这种方法形成的词向量的长度为字典的长度，每个值都是当前位置代表的词组出现的次数。使用可以过滤一部分低频率的词组来降低字典维度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clean the phrases and vectorize the sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参考代码，refer to https://thinkingneuron.com/how-to-generate-n-grams-in-python/\n",
    "# # Creating a function to generate N-Grams\n",
    "# def generate_ngrams(text, WordsToCombine):\n",
    "#      words = text.split()\n",
    "#      output = []  \n",
    "#      for i in range(len(words)- WordsToCombine+1):\n",
    "#          output.append(words[i:i+WordsToCombine])\n",
    "#      return output\n",
    " \n",
    "# # Calling the function\n",
    "# generate_ngrams(text='this is a very good book to study', WordsToCombine=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['escapadesdemonstrating', 'demonstratingadage', 'adagegood', 'goodgoose']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def tokenization_to_ngram(sentence,n=1):\n",
    "    '''\n",
    "    将句子转化为token,去除停止词,并返回用于n-gram语言建模的特征\n",
    "    '''\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = re.sub(\"[^\\w]\", \" \",  sentence).split() \n",
    "    filtered_sentence=[w.lower() for w in words if not w in stop_words]\n",
    "    output=[]\n",
    "    if(n!=1):\n",
    "        if(len(filtered_sentence)<n):\n",
    "            # 对于n>len(filtered_sentence)的情况，直接将句子中所有的词拼接\n",
    "            tmp=''\n",
    "            for i in range(len(filtered_sentence)):\n",
    "                tmp=tmp+filtered_sentence[i]\n",
    "            output.append(tmp)\n",
    "            return output\n",
    "        else:\n",
    "            # 对于其他情况，将句子分解为n个词一份\n",
    "            for i in range(len(filtered_sentence)-n+1):\n",
    "                # 这一步是将n个单词拼在一起作为一个单词，这样的话可以视作一个单词，方便一会儿进行哈希\n",
    "                tmp=filtered_sentence[i]\n",
    "                for t in range(1,n):\n",
    "                    tmp+=filtered_sentence[i+t]\n",
    "                output.append(tmp)\n",
    "            return output\n",
    "    else:\n",
    "        return filtered_sentence\n",
    "\n",
    "tokenization_to_ngram(train[\"Phrase\"][7],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictionarymake(df,colums):\n",
    "    '''制作字典'''\n",
    "    corpus=[]\n",
    "    for ls in df[colums]:\n",
    "        for i in range(len(ls)):\n",
    "            if( ls[i] not in corpus):\n",
    "                corpus.append(ls[i])\n",
    "    \n",
    "    dictionary={}\n",
    "    for i,word in enumerate(corpus):\n",
    "        dictionary[word]=i\n",
    "\n",
    "    return dictionary,len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(ls,dict,vector_len):\n",
    "    '''将句子根据字典转化为定长向量'''\n",
    "    # 使用16位浮点数，防止太长爆掉\n",
    "    vec=np.zeros(vector_len,dtype=np.float16)\n",
    "    for w in ls:\n",
    "        vec[dict[w]]+=1\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### softmax\n",
    "\n",
    "Logstic回归是softmax在2分类下的特殊情况，所以我们仅实现softmxa。\n",
    "\n",
    "由于是分类任务，损失函数使用交叉熵。\n",
    "\n",
    "使用随机梯度下降进行优化，尽管softmax有闭式解，但是这里的矩阵太大，最好使用梯度下降的方法优化。\n",
    "\n",
    "对于softmax/logistics回归来说，假设$c_k$表示类别k，X表示输入，$W_i$表示每个类别对应的权重，则其公式如下所示：\n",
    "\n",
    "$P(C=c_k |x=X)=softmax(X)=\\frac{e^{W_{k}^TX}}{\\sum_{i=1}^{K}{e^{W_{i}^TX}}}$\n",
    "\n",
    "对于Logistics回归，另一种更严谨的公式是这样的（注意分母没有第K类,此时的公式由两两分类下的对数线性回归严格的推出）：\n",
    "\n",
    " + 对于第K类$P(C=c_K |x=X)=logistics(X)=\\frac{1}{1+\\sum_{i=1}^{K-1}{e^{W_{i}^TX}}}$\n",
    " + 对于其余的K-1类，$P(C=c_{k-1} |x=X)=logistics(X)=\\frac{e^{W_{k-1}^TX}}{1+\\sum_{i=1}^{K-1}{e^{W_{i}^TX}}}$\n",
    "\n",
    "这是由于下面的两个公式没有同时在分子和分母上乘以$e^{W_{k}^TX}$，由于分子分母同时变大，所以对最终的拟合结果并无影响。同时，上面的式子要对**K**个类别计算参数W，而下面的式子每次分类时需要对**K-1**个参数进行计算。\n",
    "\n",
    "虽然如此，这并不是说下面的这个式子计算的参数量要少，两者其实计算的参数量是一致的。问题就在于如果采用了下面的这个式子进行计算，我们需要对每个类别计算K-1个参数，每两个类之间的参数一定有K-2个重合，同时每个类都保留一个自己独特的参数。这样两两对比K次，我们就得到了我们所需要的K个独特的参数，这个参数量和上面的式子保持一致。\n",
    "\n",
    "可以看到上面的这个公式由于更加通用，更加的便于计算，所以更加常见，但是正式的logistics回归其实是下面的这种形式。\n",
    "\n",
    "参考链接：https://esl.hohoweiya.xyz/04-Linear-Methods-for-Classification/4.4-Logistic-Regression/index.html#_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, RepeatedStratifiedKFold\n",
    "\n",
    "from numpy.random import shuffle\n",
    "\n",
    "# 使用softmax进行分类\n",
    "class softmax():\n",
    "    def __init__(self,a=0.1,batchsize=64,iteration=1,eval_size=0.1) -> None:\n",
    "        '''\n",
    "        X->(n,m):n个m维样例 \n",
    "        y->(n,k):n个样例,每个样例中有K个值,对应K个类别\n",
    "        w->(m,k):将x映射到k个类别中\n",
    "        a:学习率\n",
    "        eval_size:验证集的尺寸\n",
    "        iteration:迭代次数\n",
    "        batchsize:单个batch的大小\n",
    "        '''\n",
    "        self.w=None\n",
    "        self.a=a\n",
    "        self.eva=eval_size\n",
    "        self.batchsize=batchsize\n",
    "        self.iter=iteration\n",
    "\n",
    "    def softmax_fun(self,X):\n",
    "        tmp=X@self.w\n",
    "        # 防止指数计算得到的结果过大\n",
    "        tmp-=np.max(tmp,axis=1,keepdims=True)\n",
    "        k=np.exp(tmp)/np.sum(np.exp(tmp),axis=1,keepdims=True)\n",
    "        return k\n",
    "\n",
    "    def fit(self,X,y):\n",
    "        '''根据要求使用mini-batch的梯度下降进行优化'''\n",
    "        n,m=X.shape\n",
    "        _,k=y.shape\n",
    "        \n",
    "        # 将X增广一列，以训练bias\n",
    "        X_aug=np.concatenate((X,np.ones((n,1))),axis=1)\n",
    "        self.w=np.random.randn(m+1,k)\n",
    "        \n",
    "        # mini-batch SGD\n",
    "        batch=len(X_aug)//self.batchsize\n",
    "        for it in range(self.iter):\n",
    "            for i in range(batch):\n",
    "                # 注意越界\n",
    "                if(i!=batch-1):\n",
    "                    begin=i*self.batchsize\n",
    "                    end=(i+1)*self.batchsize\n",
    "                else:\n",
    "                    # 最后一个batch，单独处理防止越界\n",
    "                    begin=i*self.batchsize\n",
    "                    end=len(X_aug)\n",
    "                batch_x=X_aug[begin:end]\n",
    "                batch_y=y[begin:end]\n",
    "                # SGD\n",
    "                y_hat=self.softmax_fun(batch_x)\n",
    "                self.w+=self.a*batch_x.T*(batch_y-y_hat)\n",
    "\n",
    "    def predict(self,X):\n",
    "        # 增广一列，加入bias\n",
    "        a,b=X.shape\n",
    "        X_aug=np.concatenate((X,np.ones((a,1))),axis=1)\n",
    "        print(X_aug.shape)\n",
    "        return self.softmax_fun(X_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection(dict,length,phrase_set,columns):\n",
    "    '''\n",
    "    dict:字典\n",
    "    len:字典长度\n",
    "    phrase_set:语料库\n",
    "    '''\n",
    "    corpus=np.empty(length)\n",
    "    tmp=np.zeros(length)\n",
    "    for sentence in phrase_set[columns]:\n",
    "        for w in sentence:\n",
    "            # 计算w出现的次数\n",
    "            tmp[dict[w]]+=1\n",
    "            # 记录在当前位置出现的w本体\n",
    "            corpus[dict[w]]=w\n",
    "    \n",
    "    l=0\n",
    "    for i,count in enumerate(tmp):\n",
    "        # 出现频次少于5的词汇被剔除\n",
    "        if(count<=5):\n",
    "            dict.pop(corpus[i])\n",
    "            l+=1\n",
    "    return dict, length-l \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# all_phrase_1=pd.concat([train[\"Phrase\"],test[\"Phrase\"]],axis=0,ignore_index=True)\\\n",
    "#             .apply(tokenization_to_ngram,n=1)\\\n",
    "#             .rename(columns={\"Phrase\":\"allphrase\"})\n",
    "# X_train_1=pd.DataFrame(train[\"Phrase\"],columns=[\"newphrase\"]).apply(tokenization_to_ngram,n=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_1,len_v1=dictionarymake(all_phrase_1,colums=\"allphrase\")\n",
    "# dict_1,len_v1=feature_selection(dict_1,len_v1,all_phrase_1,columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# X1=X_train_1[\"newphrase\"].apply(vectorize,dict=dict_1,vector_len=len_v1)\n",
    "# X1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del X1\n",
    "# del dict_1\n",
    "# del X_train_1\n",
    "# del all_phrase_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_phrase_2=pd.concat([train[\"Phrase\"],test[\"Phrase\"]],axis=0,ignore_index=True)\\\n",
    "            .apply(tokenization_to_ngram,n=2)\\\n",
    "            .rename(columns={\"Phrase\":\"allphrase\"})\n",
    "X_train_2=pd.DataFrame(train[\"Phrase\"],columns=[\"newphrase\"]).apply(tokenization_to_ngram,n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_2,len_v2=dictionarymake(all_phrase_2,colums=\"newphrase\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2=X_train_2[\"newphrase\"].apply(vectorize,dict=dict_2,vector_len=len_v2)\n",
    "X2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将数据打乱\n",
    "# shuffle_index=np.random.permutation(len(X_aug))\n",
    "# X_=X_aug[shuffle_index]\n",
    "# y_=y[shuffle_index]\n",
    "        \n",
    "# 划分测试集和验证集,验证集不需要太大\n",
    "# train_size=int(len(X_aug)*(1-self.eva))\n",
    "# X_train,y_train=X_[:,:train_size],y_[:,:train_size]\n",
    "# X_eval,y_eval=X_[:,train_size:],y_[:,train_size:]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X2\n",
    "del dict_2\n",
    "del X_train_2\n",
    "del all_phrase_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
