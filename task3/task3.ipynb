{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据分析/处理\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 搭建神经网络\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "# 数据可视化\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "# word2vec\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x21eae1ad490>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 验证cuda是否可用\n",
    "cuda_available=torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if cuda_available else \"cpu\")\n",
    "if cuda_available:\n",
    "    print(\"CUDA Device Name:\", torch.cuda.get_device_name(0))\n",
    "    print(\"CUDA Compute Capability:\", torch.cuda.get_device_capability(0))\n",
    "# 宇宙的答案\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据集  \n",
    "这里使用的数据集是斯坦福大学提供的SNLI数据集，这个数据集中有两种可以处理的数据类型。  \n",
    "+ json\n",
    "+ txt\n",
    "\n",
    "在本数据集中，两类文件部分内容有差异，但是我们所需要的两个句子和标签在两类文件中是完全一样的。也就是说，我们仅需要载入json或者txt两种格式中的一种来进行训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotator_labels</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[neutral]</td>\n",
       "      <td>A person on a horse jumps over a broken down a...</td>\n",
       "      <td>A person is training his horse for a competition.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[contradiction]</td>\n",
       "      <td>A person on a horse jumps over a broken down a...</td>\n",
       "      <td>A person is at a diner, ordering an omelette.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[entailment]</td>\n",
       "      <td>A person on a horse jumps over a broken down a...</td>\n",
       "      <td>A person is outdoors, on a horse.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[neutral]</td>\n",
       "      <td>Children smiling and waving at camera</td>\n",
       "      <td>They are smiling at their parents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[entailment]</td>\n",
       "      <td>Children smiling and waving at camera</td>\n",
       "      <td>There are children present</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  annotator_labels                                          sentence1  \\\n",
       "0        [neutral]  A person on a horse jumps over a broken down a...   \n",
       "1  [contradiction]  A person on a horse jumps over a broken down a...   \n",
       "2     [entailment]  A person on a horse jumps over a broken down a...   \n",
       "3        [neutral]              Children smiling and waving at camera   \n",
       "4     [entailment]              Children smiling and waving at camera   \n",
       "\n",
       "                                           sentence2  \n",
       "0  A person is training his horse for a competition.  \n",
       "1      A person is at a diner, ordering an omelette.  \n",
       "2                  A person is outdoors, on a horse.  \n",
       "3                  They are smiling at their parents  \n",
       "4                         There are children present  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use txt\n",
    "# train_data=pd.read_csv(\"snli_1.0\\snli_1.0_train.txt\",delimiter='\\t')\n",
    "# use json\n",
    "train_data=pd.read_json(\"snli_1.0\\snli_1.0_train.jsonl\",lines=True)\n",
    "train_data=pd.concat([train_data[\"annotator_labels\"],train_data[\"sentence1\"],train_data[\"sentence2\"]],axis=1)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from task1.ipynb\n",
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def tokenization_to_ngram(sentence,n=1):\n",
    "    '''\n",
    "    将句子转化为token,去除停止词,并返回用于n-gram语言建模的特征\n",
    "    '''\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # print(type(sentence))\n",
    "    words = re.sub(\"[^\\w]\", \" \",  sentence).split() \n",
    "    filtered_sentence=[w.lower() for w in words if not w in stop_words]\n",
    "    output=[]\n",
    "    if(n!=1):\n",
    "        if(len(filtered_sentence)<n):\n",
    "            # 对于n>len(filtered_sentence)的情况，直接将句子中所有的词拼接\n",
    "            tmp=''\n",
    "            for i in range(len(filtered_sentence)):\n",
    "                tmp=tmp+filtered_sentence[i]\n",
    "            output.append(tmp)\n",
    "            return output\n",
    "        else:\n",
    "            # 对于其他情况，将句子分解为n个词一份\n",
    "            for i in range(len(filtered_sentence)-n+1):\n",
    "                # 这一步是将n个单词拼在一起作为一个单词，这样的话可以视作一个单词，方便一会儿进行哈希\n",
    "                tmp=filtered_sentence[i]\n",
    "                for t in range(1,n):\n",
    "                    tmp+=filtered_sentence[i+t]\n",
    "                output.append(tmp)\n",
    "            return output\n",
    "    else:\n",
    "        return filtered_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentence_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model1\u001b[38;5;241m=\u001b[39mWord2Vec(\u001b[43msentence_list\u001b[49m,vector_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,sg\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      2\u001b[0m model1\u001b[38;5;241m.\u001b[39mvector_size\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sentence_list' is not defined"
     ]
    }
   ],
   "source": [
    "model1=Word2Vec(,vector_size=50,sg=1)\n",
    "model1.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sen2word2vec(sentence_list,model=model1,maxlen=maxSenLen):\n",
    "    vecList=np.zeros((maxlen,model.vector_size)) \n",
    "    if(sentence_list==[]):\n",
    "        return vecList \n",
    "    # 将列表句子转化为稠密词向量句子\n",
    "    # vecList=np.array([])\n",
    "    for i,e in enumerate(sentence_list):\n",
    "        if(e in model.wv):\n",
    "            vecList[i]=model.wv[e]\n",
    "    return vecList\n",
    "\n",
    "s=['a','good','kjell']\n",
    "# print(model1.wv[s])\n",
    "p=sen2word2vec(s,model1)\n",
    "print(p[0]==model1.wv['a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
