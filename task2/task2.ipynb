{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据分析/处理\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 搭建神经网络\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import torchtext\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "# 数据可视化\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "# word2vec\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2897fd3bd68>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 宇宙的答案\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 关于词嵌入（Word Embedding）\n",
    "\n",
    "词嵌入（Word Embedding）指的是根据词汇在文本中的上下文将词汇转化为稠密化的向量（也称为分布式表示）表示的一类算法。之所以称这种技术为嵌入（Embedding）是由于它可以将词汇表示在一个向量空间中，我们甚至可以使用这些表示（一般简称为词向量）进行一些带有语义的运算（例如：king-man+woman=queen，或计算余弦相似度）。最早的基于DL的词嵌入技术Word2Vec由传奇捷克NLP研究员Tomas Mikolov于2013年前后与Ilya、Jeff Dean等人于Google共同开发。\n",
    "\n",
    "Word2Vec的基本假设就是：每个词汇的含义，取决于它**可能**出现的上下文（事后证明这个假设基本上是正确的）。Word2Vec突破了之前的各种词汇表示方法无法联系上下文的缺点,可以显示词之间的相似关系，且稠密的向量化的表示更适于计算和存储。但是Word2Vec仍然有一些缺点，例如由于反义词由于出现的语境比较接近，所以两个反义词在向量空间中比较接近，正常来讲一对反义词的词向量应该成一个平角才对。\n",
    "\n",
    "GloVe则是斯坦福大学提出的针对Word2Vec的改进，相较于Word2Vec改进了面对生僻词时的等情况下的性能（尽管Mikolov仍然认为GloVe的效果逊于Word2Vec）。\n",
    "\n",
    "根据原项目的要求，应当使用以下三种方式对词汇进行表示：\n",
    " + Word2Vec（对应原要求中的word embedding，这样处理是因为Word2Vec影响力太大，所以有时有人会用word embedding指代它）\n",
    " + 随机初始化\n",
    " + GloVe\n",
    "\n",
    "然后搭建RNN和CNN进行分类。\n",
    "\n",
    "欲了解更多可以参考以下资料：\n",
    "\n",
    "[Word2Vec的介绍1](https://zhuanlan.zhihu.com/p/61635013)\n",
    "\n",
    "[Word2Vec的介绍2](https://zhuanlan.zhihu.com/p/26306795)\n",
    "\n",
    "[GloVe的介绍1](https://zhuanlan.zhihu.com/p/50946044)\n",
    "\n",
    "[GloVe的介绍2](https://zhuanlan.zhihu.com/p/42073620)\n",
    "\n",
    "[GloVe的介绍3](https://zh.gluon.ai/chapter_natural-language-processing/glove.html)\n",
    "\n",
    "另外，Word2Vec和GloVe并不是唯一的一种分布式词向量表示方法。另一类常用的分布式词表示方法还有ELMo和Bert，这类方法可以通过考察词语所在的上下文语境动态的形成词向量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 载入数据\\处理数据\n",
    "\n",
    "对数据的分析同task1 \n",
    "\n",
    "Word2Vec使用Gensim库内置的实现，在我们自己的语料库中进行训练\n",
    "\n",
    "随机初始化使用torch原生的随机化Embedding\n",
    "\n",
    "GloVe使用斯坦福原生的预训练权重\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 156060 entries, 0 to 156059\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count   Dtype \n",
      "---  ------     --------------   ----- \n",
      " 0   Phrase     156060 non-null  object\n",
      " 1   Sentiment  156060 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.4+ MB\n"
     ]
    }
   ],
   "source": [
    "# 数据在task1文件架中\n",
    "test=pd.read_csv(\"../task1/test.tsv\", delimiter=\"\\t\").drop(columns=[\"PhraseId\",\"SentenceId\"])\n",
    "train=pd.read_csv(\"../task1/train.tsv\", delimiter=\"\\t\").drop(columns=[\"PhraseId\",\"SentenceId\"])\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "283\n"
     ]
    }
   ],
   "source": [
    "maxSenLen=0\n",
    "for i in train[\"Phrase\"].to_list():\n",
    "    maxSenLen=max(maxSenLen,len(i))\n",
    "print(maxSenLen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from task1.ipynb\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def tokenization_to_ngram(sentence,n=1):\n",
    "    '''\n",
    "    将句子转化为token,去除停止词,并返回用于n-gram语言建模的特征\n",
    "    '''\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # print(type(sentence))\n",
    "    words = re.sub(\"[^\\w]\", \" \",  sentence).split() \n",
    "    filtered_sentence=[w.lower() for w in words if not w in stop_words]\n",
    "    output=[]\n",
    "    if(n!=1):\n",
    "        if(len(filtered_sentence)<n):\n",
    "            # 对于n>len(filtered_sentence)的情况，直接将句子中所有的词拼接\n",
    "            tmp=''\n",
    "            for i in range(len(filtered_sentence)):\n",
    "                tmp=tmp+filtered_sentence[i]\n",
    "            output.append(tmp)\n",
    "            return output\n",
    "        else:\n",
    "            # 对于其他情况，将句子分解为n个词一份\n",
    "            for i in range(len(filtered_sentence)-n+1):\n",
    "                # 这一步是将n个单词拼在一起作为一个单词，这样的话可以视作一个单词，方便一会儿进行哈希\n",
    "                tmp=filtered_sentence[i]\n",
    "                for t in range(1,n):\n",
    "                    tmp+=filtered_sentence[i+t]\n",
    "                output.append(tmp)\n",
    "            return output\n",
    "    else:\n",
    "        return filtered_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'series',\n",
       " 'escapades',\n",
       " 'demonstrating',\n",
       " 'adage',\n",
       " 'good',\n",
       " 'goose',\n",
       " 'also',\n",
       " 'good',\n",
       " 'gander',\n",
       " 'occasionally',\n",
       " 'amuses',\n",
       " 'none',\n",
       " 'amounts',\n",
       " 'much',\n",
       " 'story']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将句子转化为列表\n",
    "sentence_list=pd.concat([train[\"Phrase\"],test[\"Phrase\"]],axis=0,ignore_index=True)\\\n",
    "                .apply(tokenization_to_ngram,n=1).to_list()\n",
    "sentence_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据集类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建模  \n",
    "使用CNN和RNN进行分类，使用Dropout防止过拟合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN\n",
    "\n",
    "参考：[pytorch官方文档](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,out_size,num_layers=1,bid=False,dropout=0.1) -> None:\n",
    "        super(RNN,self).__init__()\n",
    "        self.input_size=input_size\n",
    "        self.hidden_size=hidden_size\n",
    "        self.num_layers=num_layers\n",
    "        self.bid=bid\n",
    "        self.rnn=nn.RNN(input_size,hidden_size,\n",
    "                        num_layers,batch_first=True,dropout=dropout,bidirectional=bid)\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        self.project=nn.Linear(hidden_size,out_size)\n",
    "    \n",
    "    def forward(self,X):\n",
    "        # deft: self.bid*self.num_layers=1, batch_size=X.size(0) \n",
    "        h0=torch.zeros(self.bid*self.num_layers,X.size(0),self.hidden_size)\n",
    "        X,_=self.rnn(X,h0)\n",
    "        X=self.dropout(X)\n",
    "        output=self.project(X)\n",
    "        return output "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(TextCNN).__init__()\n",
    "\n",
    "    def forward(self,X):\n",
    "        return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 词向量维度为50维,sg=1表示使用skip-gram\n",
    "model1=Word2Vec(sentence_list,vector_size=50,sg=1)\n",
    "model1.save(\"Word2Vec.task2.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(283, 50)\n"
     ]
    }
   ],
   "source": [
    "def sen2word2vec(sentence_list,model,maxlen=maxSenLen):\n",
    "    vecList=model.wv[sentence_list]\n",
    "    l,_=vecList.shape\n",
    "    if(l==maxlen):\n",
    "        return vecList\n",
    "    else:\n",
    "        for i in range(maxSenLen-l):\n",
    "            z=np.zeros((1,50))\n",
    "            vecList=np.append(vecList,z,axis=0)\n",
    "    return vecList\n",
    "s=['a','none','good']\n",
    "p=sen2word2vec(s,model1)\n",
    "print(p.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 随机向量初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomEmbedding(nn.Module):\n",
    "    def __init__(self,vocab_size,d_model) -> None:\n",
    "        super(RandomEmbedding,self).__init__()\n",
    "        self.embedding=nn.Embedding()\n",
    "    def forward(self,X):\n",
    "        return self.embedding(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train\n",
    "del test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
