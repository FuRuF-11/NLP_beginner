{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据分析/处理\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 搭建神经网络\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "# 数据可视化\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "# word2vec\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x227acabb330>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 宇宙的答案\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 关于词嵌入（Word Embedding）\n",
    "\n",
    "词嵌入（Word Embedding）指的是根据词汇在文本中的上下文将词汇转化为稠密化的向量（也称为分布式表示）表示的一类算法。之所以称这种技术为嵌入（Embedding）是由于它可以将词汇表示在一个向量空间中，我们甚至可以使用这些表示（一般简称为词向量）进行一些带有语义的运算（例如：king-man+woman=queen，或计算余弦相似度）。最早的基于DL的词嵌入技术Word2Vec由传奇捷克NLP研究员Tomas Mikolov于2013年前后与Ilya、Jeff Dean等人于Google共同开发。\n",
    "\n",
    "Word2Vec的基本假设就是：每个词汇的含义，取决于它**可能**出现的上下文（事后证明这个假设基本上是正确的）。Word2Vec突破了之前的各种词汇表示方法无法联系上下文的缺点,可以显示词之间的相似关系，且稠密的向量化的表示更适于计算和存储。但是Word2Vec仍然有一些缺点，例如由于反义词由于出现的语境比较接近，所以两个反义词在向量空间中比较接近，正常来讲一对反义词的词向量应该成一个平角才对。\n",
    "\n",
    "GloVe则是斯坦福大学提出的针对Word2Vec的改进，相较于Word2Vec改进了面对生僻词时的等情况下的性能（尽管Mikolov仍然认为GloVe的效果逊于Word2Vec）。\n",
    "\n",
    "根据原项目的要求，应当使用以下三种方式对词汇进行表示：\n",
    " + Word2Vec（对应原要求中的word embedding，这样处理是因为Word2Vec影响力太大，所以有时有人会用word embedding指代它）\n",
    " + 随机初始化\n",
    " + GloVe\n",
    "\n",
    "然后搭建RNN和CNN进行分类。\n",
    "\n",
    "欲了解更多可以参考以下资料：\n",
    "\n",
    "[Word2Vec的介绍1](https://zhuanlan.zhihu.com/p/61635013)\n",
    "\n",
    "[Word2Vec的介绍2](https://zhuanlan.zhihu.com/p/26306795)\n",
    "\n",
    "[GloVe的介绍1](https://zhuanlan.zhihu.com/p/50946044)\n",
    "\n",
    "[GloVe的介绍2](https://zhuanlan.zhihu.com/p/42073620)\n",
    "\n",
    "[GloVe的介绍3](https://zh.gluon.ai/chapter_natural-language-processing/glove.html)\n",
    "\n",
    "另外，Word2Vec和GloVe并不是唯一的一种分布式词向量表示方法。另一类常用的分布式词表示方法还有ELMo和Bert，这类方法可以通过考察词语所在的上下文语境动态的形成词向量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 载入数据\\处理数据\n",
    "\n",
    "对数据的分析同task1 \n",
    "\n",
    "Word2Vec使用Gensim库内置的实现，在我们自己的语料库中进行训练\n",
    "\n",
    "随机初始化使用torch原生的随机化Embedding\n",
    "\n",
    "GloVe使用斯坦福原生的预训练权重\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 156060 entries, 0 to 156059\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count   Dtype \n",
      "---  ------     --------------   ----- \n",
      " 0   Phrase     156060 non-null  object\n",
      " 1   Sentiment  156060 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.4+ MB\n"
     ]
    }
   ],
   "source": [
    "# 数据在task1文件架中\n",
    "test=pd.read_csv(\"../task1/test.tsv\", delimiter=\"\\t\").drop(columns=[\"PhraseId\",\"SentenceId\"])\n",
    "train=pd.read_csv(\"../task1/train.tsv\", delimiter=\"\\t\").drop(columns=[\"PhraseId\",\"SentenceId\"])\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "283\n"
     ]
    }
   ],
   "source": [
    "maxSenLen=0\n",
    "for i in train[\"Phrase\"].to_list():\n",
    "    maxSenLen=max(maxSenLen,len(i))\n",
    "print(maxSenLen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from task1.ipynb\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def tokenization_to_ngram(sentence,n=1):\n",
    "    '''\n",
    "    将句子转化为token,去除停止词,并返回用于n-gram语言建模的特征\n",
    "    '''\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # print(type(sentence))\n",
    "    words = re.sub(\"[^\\w]\", \" \",  sentence).split() \n",
    "    filtered_sentence=[w.lower() for w in words if not w in stop_words]\n",
    "    output=[]\n",
    "    if(n!=1):\n",
    "        if(len(filtered_sentence)<n):\n",
    "            # 对于n>len(filtered_sentence)的情况，直接将句子中所有的词拼接\n",
    "            tmp=''\n",
    "            for i in range(len(filtered_sentence)):\n",
    "                tmp=tmp+filtered_sentence[i]\n",
    "            output.append(tmp)\n",
    "            return output\n",
    "        else:\n",
    "            # 对于其他情况，将句子分解为n个词一份\n",
    "            for i in range(len(filtered_sentence)-n+1):\n",
    "                # 这一步是将n个单词拼在一起作为一个单词，这样的话可以视作一个单词，方便一会儿进行哈希\n",
    "                tmp=filtered_sentence[i]\n",
    "                for t in range(1,n):\n",
    "                    tmp+=filtered_sentence[i+t]\n",
    "                output.append(tmp)\n",
    "            return output\n",
    "    else:\n",
    "        return filtered_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'series',\n",
       " 'escapades',\n",
       " 'demonstrating',\n",
       " 'adage',\n",
       " 'good',\n",
       " 'goose',\n",
       " 'also',\n",
       " 'good',\n",
       " 'gander',\n",
       " 'occasionally',\n",
       " 'amuses',\n",
       " 'none',\n",
       " 'amounts',\n",
       " 'much',\n",
       " 'story']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将句子转化为列表\n",
    "# sentence_list=pd.concat([train[\"Phrase\"],test[\"Phrase\"]],axis=0,ignore_index=True)\\\n",
    "#                 .apply(tokenization_to_ngram,n=1).to_list()\n",
    "sentence_list=train[\"Phrase\"].apply(tokenization_to_ngram,n=1).to_list()\n",
    "sentence_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test 数据集没有标签，我们使用将train分割成我们需要的大小\n",
    "data,label=\\\n",
    "    train[\"Phrase\"].apply(tokenization_to_ngram,n=1).to_list(),\\\n",
    "    train[\"Sentiment\"].to_list()\n",
    "test_size=int(0.1*len(data))\n",
    "train_size=len(data)-test_size\n",
    "train_data,train_label=data[:train_size],label[:train_size]\n",
    "test_data,test_label=data[train_size:],label[train_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建模  \n",
    "使用CNN和RNN进行分类，使用Dropout防止过拟合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN\n",
    "\n",
    "参考：[pytorch官方文档](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self,hidden_size,out_size,d_model=50,num_layers=1,bid=False,dropout=0.1) -> None:\n",
    "        super(RNN,self).__init__()\n",
    "        # h0的维度\n",
    "        self.hidden_size=hidden_size\n",
    "        # 有几层，一般来说，有一层或者两层就足够了\n",
    "        self.num_layers=num_layers\n",
    "        # 是否是双向RNN\n",
    "        self.bid=bid\n",
    "        self.rnn=nn.RNN(d_model,hidden_size,\n",
    "                        num_layers,batch_first=True,dropout=dropout,bidirectional=bid)\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        self.project=nn.Sequential(\n",
    "            nn.Linear(hidden_size,out_size),\n",
    "            nn.Softmax()\n",
    "            )\n",
    "    def forward(self,X):\n",
    "        # deft: self.bid*self.num_layers=1, batch_size=X.size(0) \n",
    "        if self.bid==True:\n",
    "            h0=torch.zeros(2*self.num_layers,X.size(0),self.hidden_size)\n",
    "        else:\n",
    "            h0=torch.zeros(self.num_layers,X.size(0),self.hidden_size)\n",
    "        X,_=self.rnn(X,h0)\n",
    "        X=self.dropout(X)\n",
    "        output=self.project(X[:,-1,:])\n",
    "        return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 6])\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "a=torch.ones(3,10,5)\n",
    "rnn=RNN(5,6,5)\n",
    "out=rnn(a)\n",
    "print(out.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN\n",
    "\n",
    "参考：[TextCNN](https://arxiv.org/abs/1408.5882)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(nn.Module):\n",
    "    # TextCNN only has 3 layers\n",
    "    def __init__(self,hidden_size,out_size,d_model=50,kernel_size=[3,4,5],dropout=0.1) -> None:\n",
    "        super(TextCNN,self).__init__()\n",
    "        self.hidden_size=hidden_size\n",
    "        self.d_model=d_model\n",
    "        self.ConvBlocks=nn.ModuleList([\n",
    "            nn.Conv2d(1,hidden_size,(i,d_model)) for i in kernel_size])\n",
    "        self.project=nn.Sequential(\n",
    "            nn.Linear(hidden_size*len(kernel_size),out_size),\n",
    "            nn.Softmax()\n",
    "            )\n",
    "        \n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self,X):\n",
    "        # [batch,sentencelen,d_model]-->[batch,1,sentencelen,d_model]\n",
    "        X=X.unsqueeze(1)\n",
    "        pool_out=[]\n",
    "        for conv in self.ConvBlocks:\n",
    "            # [batch,1,sentencelen,d_model]-->[batch,hidden_size,sentencelen,1]    \n",
    "            conv_out=F.relu(conv(X))\n",
    "            # [batch,hidden_size,sentencelen,1]-->[batch,hidden_size,1,1]\n",
    "            pool=F.max_pool2d(conv_out,(conv_out.size(2),1))\n",
    "            # [batch,hidden_size,1,1]-->[batch,hidden_size,1]\n",
    "            pool=pool.squeeze(3)\n",
    "            pool_out.append(pool)\n",
    "        # [batch,hidden_size,1]-->[batch,hidden_size*3,1]\n",
    "        X=torch.cat(pool_out,dim=1)\n",
    "        # just to make sure the size of tensor will match\n",
    "        X=X.squeeze(2)\n",
    "        X=self.dropout(X)\n",
    "        output=self.project(X)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "a=torch.ones(3,10,5)\n",
    "cnn=TextCNN(3,4,5)\n",
    "out=cnn(a)\n",
    "print(out.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 词向量维度为50维,sg=1表示使用skip-gram\n",
    "model1=Word2Vec(sentence_list,vector_size=50,sg=1)\n",
    "model1.vector_size\n",
    "# model1.save(\"Word2Vec.task2.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(283, 50)\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True]\n"
     ]
    }
   ],
   "source": [
    "def sen2word2vec(sentence_list,model=model1,maxlen=maxSenLen):\n",
    "    vecList=np.zeros((maxlen,model.vector_size)) \n",
    "    if(sentence_list==[]):\n",
    "        return vecList \n",
    "    # 将列表句子转化为稠密词向量句子\n",
    "    print(vecList.shape)\n",
    "    # vecList=np.array([])\n",
    "    for i,e in enumerate(sentence_list):\n",
    "        if(e in model.wv):\n",
    "            vecList[i]=model.wv[e]\n",
    "    return vecList\n",
    "    #     # else:\n",
    "    #     #     z=np.zeros((1,model.vector_size))\n",
    "    #     #     vecList=np.concatenate((vecList,z),axis=0)\n",
    "    # l=len(vecList)\n",
    "    # if(l==maxlen):\n",
    "    #     return vecList\n",
    "    # else:\n",
    "    #     for i in range(maxSenLen-l):\n",
    "    #         vecList=np.concatenate((vecList,z),axis=0)\n",
    "    # return np.array(vecList)\n",
    "s=['a','good','kjell']\n",
    "# print(model1.wv[s])\n",
    "p=sen2word2vec(s,model1)\n",
    "print(p[0]==model1.wv['a'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 随机向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 需要确定字典中词的个数\n",
    "# 较为简单的处理方法，直接设定一个较大的数字作为估计\n",
    "v_size=20000\n",
    "# 随后需要对每个单词都进行编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomEmbedding(nn.Module):\n",
    "    def __init__(self,vocab_size=v_size,d_model=50,sentence_len=maxSenLen) -> None:\n",
    "        super(RandomEmbedding,self).__init__()\n",
    "        self.senLen=sentence_len\n",
    "        self.d_model=d_model\n",
    "        self.embedding=nn.Embedding(vocab_size,d_model)\n",
    "        self.embedding.weight.data.normal_(mean=0,std=1)\n",
    "    def forward(self,X):\n",
    "        context=self.embedding(X)\n",
    "        z=torch.zeros(1,50)\n",
    "        # if(context.size(0)<self.senLen):\n",
    "        l,_=context.size(0)\n",
    "        if(l==self.senLen):\n",
    "            return context\n",
    "        else:\n",
    "            for i in range(self.senLen-l):\n",
    "                z=torch.zeros((1,self.d_model))\n",
    "                context=torch.cat((context,z),dim=0)\n",
    "            return context\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(283, 50)\n"
     ]
    }
   ],
   "source": [
    "import torchtext\n",
    "\n",
    "def sen2GloVe(sentence_list,model='6B',d_model=50,maxlen=maxSenLen):\n",
    "    g=torchtext.vocab.GloVe(model,d_model)\n",
    "    s=np.array([np.array(g[word]) for word in sentence_list])\n",
    "    if(len(s)==maxlen):\n",
    "        return np.array(s)\n",
    "    else:\n",
    "        for _ in range(maxlen-len(s)):\n",
    "            z=np.zeros((1,d_model))\n",
    "            s=np.concatenate((s,z),axis=0)\n",
    "        return np.array(s)\n",
    "s=['a','good','flim']\n",
    "p=sen2GloVe(s)\n",
    "print(p.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据集类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDateset(Dataset):\n",
    "    def __init__(self,data,label,model='w') -> None:\n",
    "        '''\n",
    "        model='w'/'r'/'g',means word2vec/random/GloVe\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.data=data\n",
    "        self.label=label\n",
    "        self.model=model\n",
    "        if(self.model=='r'):\n",
    "            self.randvec=RandomEmbedding()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        sentence=self.data[index]\n",
    "        label=self.label[index]\n",
    "        # 这不是一种高效率的写法，但是在这里使用的话还是可以的\n",
    "        if(self.model=='w'):\n",
    "            return sen2word2vec(sentence),label\n",
    "        elif(self.model=='r'):\n",
    "            return self.randvec(sentence),label\n",
    "        elif(self.model=='g'):\n",
    "            return sen2GloVe(sentence),label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def eval_model(model,test):\n",
    "    model.eval()\n",
    "    t=0\n",
    "    for x,y in test:\n",
    "        x=x.float()\n",
    "        output=model(x)\n",
    "        if(y==output.argmax()):\n",
    "            t+=1\n",
    "    print('Eval, acc: {:.4f}'.format(t/test.__len__()))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainModel(model,train,epoch=3):\n",
    "    model.train()\n",
    "    loss_fn=nn.CrossEntropyLoss()\n",
    "    optimizer=optim.SGD(model.parameters(),lr=0.00001)\n",
    "    for i in range(epoch):\n",
    "        for x,y in train:\n",
    "            x=x.float()\n",
    "            output=model(x)\n",
    "            loss=loss_fn(output,y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print('Epoch [{}/{}], Loss: {:.4f}'.format(i+1, epoch, loss.item()))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "# r,g\n",
    "embedding_models='w'\n",
    "batch_size=64\n",
    "epoch=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=MyDateset(train_data,train_label,model=embedding_models)\n",
    "train_dataloader=DataLoader(train_dataset,batch_size=batch_size,shuffle=True)\n",
    "test_dataset=MyDateset(test_data,test_label,model=embedding_models)\n",
    "test_dataloader=DataLoader(test_dataset,batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'storytellers' not present\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# RNN\u001b[39;00m\n\u001b[0;32m      2\u001b[0m rnn\u001b[38;5;241m=\u001b[39mRNN(\u001b[38;5;241m256\u001b[39m,\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m rnn\u001b[38;5;241m=\u001b[39m\u001b[43mTrainModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m eval_model(rnn,test_dataloader,epoch\u001b[38;5;241m=\u001b[39mepoch)\n",
      "Cell \u001b[1;32mIn[41], line 6\u001b[0m, in \u001b[0;36mTrainModel\u001b[1;34m(model, train, epoch)\u001b[0m\n\u001b[0;32m      4\u001b[0m optimizer\u001b[38;5;241m=\u001b[39moptim\u001b[38;5;241m.\u001b[39mSGD(model\u001b[38;5;241m.\u001b[39mparameters(),lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.00001\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epoch):\n\u001b[1;32m----> 6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x,y \u001b[38;5;129;01min\u001b[39;00m train:\n\u001b[0;32m      7\u001b[0m         x\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m      8\u001b[0m         output\u001b[38;5;241m=\u001b[39mmodel(x)\n",
      "File \u001b[1;32md:\\DataScience\\Env\\DL2024\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32md:\\DataScience\\Env\\DL2024\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32md:\\DataScience\\Env\\DL2024\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32md:\\DataScience\\Env\\DL2024\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[39], line 21\u001b[0m, in \u001b[0;36mMyDateset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# 这不是一种高效率的写法，但是在这里使用的话还是可以的\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m---> 21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msen2word2vec\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m,label\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandvec(sentence),label\n",
      "Cell \u001b[1;32mIn[35], line 8\u001b[0m, in \u001b[0;36msen2word2vec\u001b[1;34m(sentence_list, model, maxlen)\u001b[0m\n\u001b[0;32m      6\u001b[0m vecList\u001b[38;5;241m=\u001b[39m[]\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m sentence_list:\n\u001b[1;32m----> 8\u001b[0m     vecList\u001b[38;5;241m.\u001b[39mappend(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwv\u001b[49m\u001b[43m[\u001b[49m\u001b[43me\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[0;32m      9\u001b[0m l,_\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray(vecList)\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(l\u001b[38;5;241m==\u001b[39mmaxlen):\n",
      "File \u001b[1;32md:\\DataScience\\Env\\DL2024\\lib\\site-packages\\gensim\\models\\keyedvectors.py:403\u001b[0m, in \u001b[0;36mKeyedVectors.__getitem__\u001b[1;34m(self, key_or_keys)\u001b[0m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get vector representation of `key_or_keys`.\u001b[39;00m\n\u001b[0;32m    390\u001b[0m \n\u001b[0;32m    391\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    400\u001b[0m \n\u001b[0;32m    401\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key_or_keys, _KEY_TYPES):\n\u001b[1;32m--> 403\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_or_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m vstack([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_vector(key) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m key_or_keys])\n",
      "File \u001b[1;32md:\\DataScience\\Env\\DL2024\\lib\\site-packages\\gensim\\models\\keyedvectors.py:446\u001b[0m, in \u001b[0;36mKeyedVectors.get_vector\u001b[1;34m(self, key, norm)\u001b[0m\n\u001b[0;32m    422\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_vector\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    423\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the key's vector, as a 1D numpy array.\u001b[39;00m\n\u001b[0;32m    424\u001b[0m \n\u001b[0;32m    425\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    444\u001b[0m \n\u001b[0;32m    445\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 446\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m norm:\n\u001b[0;32m    448\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfill_norms()\n",
      "File \u001b[1;32md:\\DataScience\\Env\\DL2024\\lib\\site-packages\\gensim\\models\\keyedvectors.py:420\u001b[0m, in \u001b[0;36mKeyedVectors.get_index\u001b[1;34m(self, key, default)\u001b[0m\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m default\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 420\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not present\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Key 'storytellers' not present\""
     ]
    }
   ],
   "source": [
    "# RNN\n",
    "rnn=RNN(256,5)\n",
    "rnn=TrainModel(rnn,train_dataloader,epoch=epoch)\n",
    "eval_model(rnn,test_dataloader,epoch=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN\n",
    "cnn=TextCNN(256,5)\n",
    "cnn=TrainModel(cnn,train_dataloader,epoch=epoch)\n",
    "eval_model(cnn,test_dataloader,epoch=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train\n",
    "del test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
