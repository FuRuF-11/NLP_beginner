{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据分析/处理\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 搭建神经网络\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "# 数据可视化\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "# word2vec\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 验证cuda是否可用\n",
    "cuda_available=torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if cuda_available else \"cpu\")\n",
    "if cuda_available:\n",
    "    print(\"CUDA Device Name:\", torch.cuda.get_device_name(0))\n",
    "    print(\"CUDA Compute Capability:\", torch.cuda.get_device_capability(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x206c2f4a4d0>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 宇宙的答案\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 关于词嵌入（Word Embedding）\n",
    "\n",
    "词嵌入（Word Embedding）指的是根据词汇在文本中的上下文将词汇转化为稠密化的向量（也称为分布式表示）表示的一类算法。之所以称这种技术为嵌入（Embedding）是由于它可以将词汇表示在一个向量空间中，我们甚至可以使用这些表示（一般简称为词向量）进行一些带有语义的运算（例如：king-man+woman=queen，或计算余弦相似度）。最早的基于DL的词嵌入技术Word2Vec由传奇捷克NLP研究员Tomas Mikolov于2013年前后与Ilya、Jeff Dean等人于Google共同开发。\n",
    "\n",
    "Word2Vec的基本假设就是：每个词汇的含义，取决于它**可能**出现的上下文（事后证明这个假设基本上是正确的）。Word2Vec突破了之前的各种词汇表示方法无法联系上下文的缺点,可以显示词之间的相似关系，且稠密的向量化的表示更适于计算和存储。但是Word2Vec仍然有一些缺点，例如由于反义词由于出现的语境比较接近，所以两个反义词在向量空间中比较接近，正常来讲一对反义词的词向量应该成一个平角才对。\n",
    "\n",
    "GloVe则是斯坦福大学提出的针对Word2Vec的改进，相较于Word2Vec改进了面对生僻词时的等情况下的性能（尽管Mikolov仍然认为GloVe的效果逊于Word2Vec）。\n",
    "\n",
    "根据原项目的要求，应当使用以下三种方式对词汇进行表示：\n",
    " + Word2Vec（对应原要求中的word embedding，这样处理是因为Word2Vec影响力太大，所以有时有人会用word embedding指代它）\n",
    " + 随机初始化\n",
    " + GloVe\n",
    "\n",
    "然后搭建RNN和CNN进行分类。\n",
    "\n",
    "欲了解更多可以参考以下资料：\n",
    "\n",
    "[Word2Vec的介绍1](https://zhuanlan.zhihu.com/p/61635013)\n",
    "\n",
    "[Word2Vec的介绍2](https://zhuanlan.zhihu.com/p/26306795)\n",
    "\n",
    "[GloVe的介绍1](https://zhuanlan.zhihu.com/p/50946044)\n",
    "\n",
    "[GloVe的介绍2](https://zhuanlan.zhihu.com/p/42073620)\n",
    "\n",
    "[GloVe的介绍3](https://zh.gluon.ai/chapter_natural-language-processing/glove.html)\n",
    "\n",
    "另外，Word2Vec和GloVe并不是唯一的一种分布式词向量表示方法。另一类常用的分布式词表示方法还有ELMo和Bert，这类方法可以通过考察词语所在的上下文语境动态的形成词向量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 载入数据\\处理数据\n",
    "\n",
    "对数据的分析同task1 \n",
    "\n",
    "Word2Vec使用Gensim库内置的实现，在我们自己的语料库中进行训练\n",
    "\n",
    "随机初始化使用torch原生的随机化Embedding\n",
    "\n",
    "GloVe使用斯坦福原生的预训练权重\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 156060 entries, 0 to 156059\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count   Dtype \n",
      "---  ------     --------------   ----- \n",
      " 0   Phrase     156060 non-null  object\n",
      " 1   Sentiment  156060 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.4+ MB\n"
     ]
    }
   ],
   "source": [
    "# 数据在task1文件架中\n",
    "test=pd.read_csv(\"../task1/test.tsv\", delimiter=\"\\t\").drop(columns=[\"PhraseId\",\"SentenceId\"])\n",
    "train=pd.read_csv(\"../task1/train.tsv\", delimiter=\"\\t\").drop(columns=[\"PhraseId\",\"SentenceId\"])\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "283\n"
     ]
    }
   ],
   "source": [
    "maxSenLen=0\n",
    "for i in train[\"Phrase\"].to_list():\n",
    "    maxSenLen=max(maxSenLen,len(i))\n",
    "print(maxSenLen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from task1.ipynb\n",
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def tokenization_to_ngram(sentence,n=1):\n",
    "    '''\n",
    "    将句子转化为token,去除停止词,并返回用于n-gram语言建模的特征\n",
    "    '''\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # print(type(sentence))\n",
    "    words = re.sub(\"[^\\w]\", \" \",  sentence).split() \n",
    "    filtered_sentence=[w.lower() for w in words if not w in stop_words]\n",
    "    output=[]\n",
    "    if(n!=1):\n",
    "        if(len(filtered_sentence)<n):\n",
    "            # 对于n>len(filtered_sentence)的情况，直接将句子中所有的词拼接\n",
    "            tmp=''\n",
    "            for i in range(len(filtered_sentence)):\n",
    "                tmp=tmp+filtered_sentence[i]\n",
    "            output.append(tmp)\n",
    "            return output\n",
    "        else:\n",
    "            # 对于其他情况，将句子分解为n个词一份\n",
    "            for i in range(len(filtered_sentence)-n+1):\n",
    "                # 这一步是将n个单词拼在一起作为一个单词，这样的话可以视作一个单词，方便一会儿进行哈希\n",
    "                tmp=filtered_sentence[i]\n",
    "                for t in range(1,n):\n",
    "                    tmp+=filtered_sentence[i+t]\n",
    "                output.append(tmp)\n",
    "            return output\n",
    "    else:\n",
    "        return filtered_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'series',\n",
       " 'escapades',\n",
       " 'demonstrating',\n",
       " 'adage',\n",
       " 'good',\n",
       " 'goose',\n",
       " 'also',\n",
       " 'good',\n",
       " 'gander',\n",
       " 'occasionally',\n",
       " 'amuses',\n",
       " 'none',\n",
       " 'amounts',\n",
       " 'much',\n",
       " 'story']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将句子转化为列表\n",
    "# sentence_list=pd.concat([train[\"Phrase\"],test[\"Phrase\"]],axis=0,ignore_index=True)\\\n",
    "#                 .apply(tokenization_to_ngram,n=1).to_list()\n",
    "sentence_list=train[\"Phrase\"].apply(tokenization_to_ngram,n=1).to_list()\n",
    "sentence_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test 数据集没有标签，我们使用将train分割成我们需要的大小\n",
    "data,label=\\\n",
    "    train[\"Phrase\"].apply(tokenization_to_ngram,n=1).to_list(),\\\n",
    "    train[\"Sentiment\"].to_list()\n",
    "test_size=int(0.1*len(data))\n",
    "train_size=len(data)-test_size\n",
    "train_data,train_label=data[:train_size],label[:train_size]\n",
    "test_data,test_label=data[train_size:],label[train_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建模  \n",
    "使用CNN和RNN进行分类，使用Dropout防止过拟合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN\n",
    "\n",
    "参考：[pytorch官方文档](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self,hidden_size,out_size,d_model=50,num_layers=1,bid=False,dropout=0.1) -> None:\n",
    "        super(RNN,self).__init__()\n",
    "        # h0的维度\n",
    "        self.hidden_size=hidden_size\n",
    "        # 有几层，一般来说，有一层或者两层就足够了\n",
    "        self.num_layers=num_layers\n",
    "        # 是否是双向RNN\n",
    "        self.bid=bid\n",
    "        self.rnn=nn.RNN(d_model,hidden_size,\n",
    "                        num_layers,batch_first=True,dropout=dropout,bidirectional=bid)\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        self.project=nn.Sequential(\n",
    "            nn.Linear(hidden_size,out_size),\n",
    "            nn.Softmax()\n",
    "            )\n",
    "    def forward(self,X):\n",
    "        # deft: self.bid*self.num_layers=1, batch_size=X.size(0) \n",
    "        if self.bid==True:\n",
    "            h0=torch.zeros(2*self.num_layers,X.size(0),self.hidden_size).to(device)\n",
    "        else:\n",
    "            h0=torch.zeros(self.num_layers,X.size(0),self.hidden_size).to(device)\n",
    "        X,_=self.rnn(X,h0)\n",
    "        X=self.dropout(X)\n",
    "        output=self.project(X[:,-1,:])\n",
    "        return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 6])\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "a=torch.ones(3,10,5).to(device)\n",
    "rnn=RNN(5,6,5).to(device)\n",
    "out=rnn(a)\n",
    "print(out.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN\n",
    "\n",
    "参考：[TextCNN](https://arxiv.org/abs/1408.5882)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(nn.Module):\n",
    "    # TextCNN only has 3 layers\n",
    "    def __init__(self,hidden_size,out_size,d_model=50,kernel_size=[3,4,5],dropout=0.1) -> None:\n",
    "        super(TextCNN,self).__init__()\n",
    "        self.hidden_size=hidden_size\n",
    "        self.d_model=d_model\n",
    "        self.ConvBlocks=nn.ModuleList([\n",
    "            nn.Conv2d(1,hidden_size,(i,d_model)) for i in kernel_size])\n",
    "        self.project=nn.Sequential(\n",
    "            nn.Linear(hidden_size*len(kernel_size),out_size),\n",
    "            nn.Softmax()\n",
    "            )\n",
    "        \n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self,X):\n",
    "        # [batch,sentencelen,d_model]-->[batch,1,sentencelen,d_model]\n",
    "        X=X.unsqueeze(1)\n",
    "        pool_out=[]\n",
    "        for conv in self.ConvBlocks:\n",
    "            # [batch,1,sentencelen,d_model]-->[batch,hidden_size,sentencelen,1]    \n",
    "            conv_out=F.relu(conv(X))\n",
    "            # [batch,hidden_size,sentencelen,1]-->[batch,hidden_size,1,1]\n",
    "            pool=F.max_pool2d(conv_out,(conv_out.size(2),1))\n",
    "            # [batch,hidden_size,1,1]-->[batch,hidden_size,1]\n",
    "            pool=pool.squeeze(3)\n",
    "            pool_out.append(pool)\n",
    "        # [batch,hidden_size,1]-->[batch,hidden_size*3,1]\n",
    "        X=torch.cat(pool_out,dim=1)\n",
    "        # just to make sure the size of tensor will match\n",
    "        X=X.squeeze(2)\n",
    "        X=self.dropout(X)\n",
    "        output=self.project(X)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "a=torch.ones(3,10,5).to(device)\n",
    "cnn=TextCNN(3,4,5).to(device)\n",
    "out=cnn(a)\n",
    "print(out.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 词向量维度为50维,sg=1表示使用skip-gram\n",
    "model1=Word2Vec(sentence_list,vector_size=50,sg=1)\n",
    "model1.vector_size\n",
    "# model1.save(\"Word2Vec.task2.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True]\n"
     ]
    }
   ],
   "source": [
    "def sen2word2vec(sentence_list,model=model1,maxlen=maxSenLen):\n",
    "    vecList=np.zeros((maxlen,model.vector_size)) \n",
    "    if(sentence_list==[]):\n",
    "        return vecList \n",
    "    # 将列表句子转化为稠密词向量句子\n",
    "    # vecList=np.array([])\n",
    "    for i,e in enumerate(sentence_list):\n",
    "        if(e in model.wv):\n",
    "            vecList[i]=model.wv[e]\n",
    "    return vecList\n",
    "\n",
    "s=['a','good','kjell']\n",
    "# print(model1.wv[s])\n",
    "p=sen2word2vec(s,model1)\n",
    "print(p[0]==model1.wv['a'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 随机向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "15253\n"
     ]
    }
   ],
   "source": [
    "# 需要确定字典中词的个数\n",
    "# 较为简单的处理方法，直接设定一个较大的数字作为估计\n",
    "import collections\n",
    "from collections import Counter\n",
    "corpus=sentence_list\n",
    "\n",
    "def BuildVocab(corpus):\n",
    "    tmp=0\n",
    "    d={}\n",
    "    for i,s in enumerate(corpus):\n",
    "        for w in s:\n",
    "            if(w not in d):\n",
    "                d[w]=tmp\n",
    "                tmp+=1\n",
    "                     \n",
    "    l=len(d)\n",
    "    return d,l\n",
    "\n",
    "vcb,vcb_size=BuildVocab(corpus)\n",
    "print('a' in vcb)\n",
    "print(vcb_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 5, 3245]\n"
     ]
    }
   ],
   "source": [
    "def sen2num(sentence,d=vcb):\n",
    "    return [d[s] for s in sentence]\n",
    "\n",
    "s=['a','good','kjell']\n",
    "print(sen2num(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([283, 50])\n"
     ]
    }
   ],
   "source": [
    "class RandomEmbedding(nn.Module):\n",
    "    def __init__(self,vocab_size=vcb_size,d_model=50,sentence_len=maxSenLen) -> None:\n",
    "        super(RandomEmbedding,self).__init__()\n",
    "        self.senLen=sentence_len\n",
    "        self.d_model=d_model\n",
    "        self.embedding=nn.Embedding(vocab_size,d_model)\n",
    "        # self.embedding.weight.data.normal_(mean=0,std=1) \n",
    "\n",
    "    def forward(self,X):\n",
    "        # context=self.embedding(X)\n",
    "        X=sen2num(X)\n",
    "        vecList=torch.zeros((self.senLen,self.d_model))\n",
    "        if(X==[]):\n",
    "            return vecList \n",
    "        for i,w in enumerate(X):\n",
    "            vecList[i]=self.embedding(torch.tensor(w))\n",
    "        return vecList \n",
    "\n",
    "s=['a','good','kjell']\n",
    "test=RandomEmbedding()\n",
    "tmp=test(s)\n",
    "print(tmp.size())\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([283, 50])\n"
     ]
    }
   ],
   "source": [
    "import torchtext\n",
    "\n",
    "g=torchtext.vocab.GloVe('6B',50)\n",
    "def sen2GloVe(sentence_list,d_model=50,maxlen=maxSenLen):\n",
    "    vecList=torch.zeros((maxlen,d_model))\n",
    "    if(sentence_list==[]):\n",
    "        return vecList\n",
    "    for i,w in enumerate(sentence_list):\n",
    "        vecList[i]=g[w]\n",
    "    return vecList\n",
    "    \n",
    "s=['a','good','flim']\n",
    "p=sen2GloVe(s)\n",
    "print(p.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据集类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDateset(Dataset):\n",
    "    def __init__(self,data,label,model='w') -> None:\n",
    "        '''\n",
    "        model='w'/'r'/'g',means word2vec/random/GloVe\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.data=data\n",
    "        self.label=label\n",
    "        self.model=model\n",
    "        if(self.model=='r'):\n",
    "            self.randvec=RandomEmbedding()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        sentence=self.data[index]\n",
    "        label=self.label[index]\n",
    "        \n",
    "        if self.model=='w': \n",
    "            return sen2word2vec(sentence),label \n",
    "        elif self.model==\"g\":   \n",
    "            return sen2GloVe(sentence),label\n",
    "        return self.randvec(sentence),label\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def eval_model(model,test):\n",
    "    model.eval()\n",
    "    t=0\n",
    "    for x,y in test:\n",
    "        x=x.float()\n",
    "        x,y=x.to(device),y.to(device)\n",
    "        output=model(x)\n",
    "        if(y==output.argmax()):\n",
    "            t+=1\n",
    "    print('Eval, acc: {:.4f}'.format(t/test.__len__()))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainModel(model,train,epoch=3):\n",
    "    model.train()\n",
    "    loss_fn=nn.CrossEntropyLoss()\n",
    "    optimizer=optim.SGD(model.parameters(),lr=0.00001)\n",
    "    for i in range(epoch):\n",
    "        for x,y in train:\n",
    "            optimizer.zero_grad()\n",
    "            x=x.float()\n",
    "            x,y=x.to(device),y.to(device)\n",
    "            output=model(x)\n",
    "            loss=loss_fn(output,y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print('Epoch [{}/{}], Loss: {:.4f}'.format(i+1, epoch, loss.item()))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "# r,g\n",
    "embedding_models='r'\n",
    "batch_size=64\n",
    "epoch=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=MyDateset(train_data,train_label,model=embedding_models)\n",
    "train_dataloader=DataLoader(train_dataset,batch_size=batch_size,shuffle=True)\n",
    "test_dataset=MyDateset(test_data,test_label,model=embedding_models)\n",
    "test_dataloader=DataLoader(test_dataset,batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN\n",
    "# rnn=RNN(256,5).to(device)\n",
    "# rnn=TrainModel(rnn,train_dataloader,epoch=epoch)\n",
    "# eval_model(rnn,test_dataloader,epoch=epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Loss: 1.5941\n",
      "Epoch [2/3], Loss: 1.6020\n",
      "Epoch [3/3], Loss: 1.6067\n",
      "Eval, acc: 0.3454\n"
     ]
    }
   ],
   "source": [
    "# CNN\n",
    "cnn=TextCNN(256,5).to(device)\n",
    "cnn=TrainModel(cnn,train_dataloader,epoch=epoch)\n",
    "eval_model(cnn,test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
