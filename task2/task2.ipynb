{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\DL\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# 数据分析/处理\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 搭建神经网络\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as function\n",
    "import torch.nn.init as init\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "# 数据可视化\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x18f4a5b5d68>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 宇宙的答案\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 关于词嵌入（Word Embedding）\n",
    "\n",
    "词嵌入（Word Embedding）指的是根据词汇在文本中的上下文将词汇转化为稠密化的向量（也称为分布式表示）表示的一类算法。之所以称这种技术为嵌入（Embedding）是由于它可以将词汇表示在一个向量空间中，我们甚至可以使用这些表示（一般简称为词向量）进行一些带有语义的运算（例如：king-man+woman=queen，或计算余弦相似度）。最早的基于DL的词嵌入技术Word2Vec由传奇捷克NLP研究员Tomas Mikolov于2013年前后与Ilya、Jeff Dean等人于Google共同开发。\n",
    "\n",
    "Word2Vec的基本假设就是：每个词汇的含义，取决于它**可能**出现的上下文（事后证明这个假设基本上是正确的）。Word2Vec突破了之前的各种词汇表示方法无法联系上下文的缺点,可以显示词之间的相似关系，且稠密的向量化的表示更适于计算和存储。但是Word2Vec仍然有一些缺点，例如由于反义词由于出现的语境比较接近，所以两个反义词在向量空间中比较接近，正常来讲一对反义词的词向量应该成一个平角才对。\n",
    "\n",
    "GloVe则是斯坦福大学提出的针对Word2Vec的改进，相较于Word2Vec改进了面对生僻词时的等情况下的性能（尽管Mikolov仍然认为GloVe的效果逊于Word2Vec）。\n",
    "\n",
    "根据原项目的要求，应当使用以下三种方式对词汇进行表示：\n",
    " + Word2Vec（对应原要求中的word embedding，这样处理是因为Word2Vec影响力太大，所以有时有人会用word embedding指代它）\n",
    " + 随机初始化\n",
    " + GloVe\n",
    "\n",
    "然后搭建RNN和CNN进行分类。\n",
    "\n",
    "欲了解更多可以参考以下资料：\n",
    "\n",
    "[Word2Vec的介绍1](https://zhuanlan.zhihu.com/p/61635013)\n",
    "\n",
    "[Word2Vec的介绍2](https://zhuanlan.zhihu.com/p/26306795)\n",
    "\n",
    "[GloVe的介绍1](https://zhuanlan.zhihu.com/p/50946044)\n",
    "\n",
    "[GloVe的介绍2](https://zhuanlan.zhihu.com/p/42073620)\n",
    "\n",
    "[GloVe的介绍3](https://zh.gluon.ai/chapter_natural-language-processing/glove.html)\n",
    "\n",
    "另外，Word2Vec和GloVe并不是唯一的一种分布式词向量表示方法。另一类常用的分布式词表示方法还有ELMo和Bert，这类方法可以通过考察词语所在的上下文语境动态的形成词向量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 载入数据，分析同task1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据在task1文件架中\n",
    "test=pd.read_csv(\"../task1/test.tsv\", delimiter=\"\\t\").drop(columns=[\"PhraseId\",\"SentenceId\"])\n",
    "train=pd.read_csv(\"../task1/train.tsv\", delimiter=\"\\t\").drop(columns=[\"PhraseId\",\"SentenceId\"])\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data(Dataset):\n",
    "    def __init__(self) -> None:\n",
    "        super(data,self).__init__()\n",
    "\n",
    "    def __getitem__(self, index) -> None:\n",
    "        return super().__getitem__(index)\n",
    "    \n",
    "\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 8])\n"
     ]
    }
   ],
   "source": [
    "class CausalConv1d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size,dilation,A=False):\n",
    "        super(CausalConv1d, self).__init__()\n",
    "        self.kernel_size=kernel_size\n",
    "        self.dilation=dilation\n",
    "        self.A=A\n",
    "        # 最优padding方式，可以根据是否是A层，来决定是否对当前的数据进行卷积\n",
    "        self.padding=(kernel_size-1)*dilation+A*1\n",
    "        self.conv1d=nn.Conv1d(in_channels,out_channels,kernel_size,\\\n",
    "                              stride=1,padding=0,dilation=dilation)\n",
    "        # self.padding = (kernel_size - 1, 0)  # 在序列的左侧进行padding，右侧不进行padding\n",
    "        # self.conv = nn.Conv1d(in_channels, out_channels, kernel_size, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.pad(x, (self.padding,0))  # 序列左侧padding\n",
    "        conv_out=self.conv1d(x)\n",
    "        if(self.A):\n",
    "            return conv_out[:,:,:-1]\n",
    "        else:\n",
    "            return conv_out\n",
    "        \n",
    "        # x = self.conv(x)\n",
    "        # return x[:, :, :-self.padding[0]]  # 去掉输出序列的右侧多余部分\n",
    "\n",
    "# 创建输入张量，形状为 [batch_size, in_channels, sequence_length]\n",
    "input = torch.randn(1, 3, 8)\n",
    "\n",
    "# 创建因果卷积层\n",
    "causal_conv = CausalConv1d(in_channels=3, out_channels=16, kernel_size=2,dilation=3)\n",
    "\n",
    "# 进行因果卷积操作\n",
    "output = causal_conv(input)\n",
    "\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train\n",
    "del test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
